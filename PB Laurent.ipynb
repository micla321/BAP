{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1RXf4aKcbpO4pycGhYTN8x7Jjibigt9We",
      "authorship_tag": "ABX9TyPuN2lbIYNHGmm2LJc6P3c5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micla321/BAP/blob/master/PB%20Laurent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n"
      ],
      "metadata": {
        "id": "NKtlDUAVpyK6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "52RkBQOfpwMo",
        "outputId": "774e2083-68aa-4c7f-fb8e-6ad2b53ce21d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            input_1       input_2        output\n",
              "count  1.000000e+05  1.000000e+05  1.000000e+05\n",
              "mean  -2.722269e+13  1.790639e+13  9.771850e+31\n",
              "std    6.916098e+15  7.063002e+15  6.513147e+32\n",
              "min   -9.758854e+16 -9.811576e+16  5.100057e-38\n",
              "25%   -2.529802e-01 -1.465311e-01  5.840816e-02\n",
              "50%    3.076774e-19  3.051135e-19  1.478026e+13\n",
              "75%    2.274812e-01  1.977993e-01  1.154886e+24\n",
              "max    9.915818e+16  9.786339e+16  9.832345e+33"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cec92d1b-77a9-4494-9083-6779c0cdd467\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_1</th>\n",
              "      <th>input_2</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.000000e+05</td>\n",
              "      <td>1.000000e+05</td>\n",
              "      <td>1.000000e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-2.722269e+13</td>\n",
              "      <td>1.790639e+13</td>\n",
              "      <td>9.771850e+31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>6.916098e+15</td>\n",
              "      <td>7.063002e+15</td>\n",
              "      <td>6.513147e+32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-9.758854e+16</td>\n",
              "      <td>-9.811576e+16</td>\n",
              "      <td>5.100057e-38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-2.529802e-01</td>\n",
              "      <td>-1.465311e-01</td>\n",
              "      <td>5.840816e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.076774e-19</td>\n",
              "      <td>3.051135e-19</td>\n",
              "      <td>1.478026e+13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.274812e-01</td>\n",
              "      <td>1.977993e-01</td>\n",
              "      <td>1.154886e+24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.915818e+16</td>\n",
              "      <td>9.786339e+16</td>\n",
              "      <td>9.832345e+33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cec92d1b-77a9-4494-9083-6779c0cdd467')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cec92d1b-77a9-4494-9083-6779c0cdd467 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cec92d1b-77a9-4494-9083-6779c0cdd467');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Specify the file path\n",
        "file_path =\"/content/drive/MyDrive/AFK_H1.xlsx\"\n",
        "\n",
        "data = pd.read_excel(file_path)\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltzT2o18rcZy",
        "outputId": "7c576bb0-9799-492c-a462-7dd0f4da63d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the input features and the output variable\n",
        "input_features = data[['input_1', 'input_2']]\n",
        "output_variable = data['output']\n",
        "\n",
        "# Perform standard scaling on the input features\n",
        "scaler = StandardScaler()\n",
        "scaled_input_features = scaler.fit_transform(input_features)\n",
        "\n",
        "# Perform logarithmic transformation on the output variable\n",
        "log_transformed_output = np.log(output_variable)\n",
        "\n",
        "# Print the transformed data\n",
        "print(stats.describe(scaled_input_features))\n",
        "print(stats.describe(log_transformed_output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNsbZ-2Sp_gm",
        "outputId": "9033d323-21be-430d-b734-80c31ca0bfdc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DescribeResult(nobs=100000, minmax=(array([-14.10648032, -13.89411451]), array([14.34130832, 13.85331256])), mean=array([-2.20978791e-17, -1.46371804e-17]), variance=array([1.00001, 1.00001]), skewness=array([-0.16695024, -0.12222463]), kurtosis=array([88.99863078, 85.9586919 ]))\n",
            "DescribeResult(nobs=100000, minmax=(-85.86898181763533, 78.27098552995528), mean=23.669658426762055, variance=1438.2794388052007, skewness=-0.5639553394944913, kurtosis=-0.5915122850270573)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data to PyTorch tensors\n",
        "input_tensor = torch.tensor(scaled_input_features, dtype=torch.float32).to(device)\n",
        "output_tensor = torch.tensor(log_transformed_output, dtype=torch.float32).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "G6D4eEDrqQT9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    input_tensor, output_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)"
      ],
      "metadata": {
        "id": "r6d_Yvi4qUyo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 500)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(500, 1)  # Additional layer\n",
        "#        self.fc3 = nn.Linear(32, 1)   # Additional layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "#        x = self.relu(x)  # Apply ReLU activation after the second layer\n",
        "#        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "model = RegressionModel(input_size=input_tensor.shape[1])\n",
        "model.to(device)  # Move the model to GPU if available\n",
        "print(model)\n",
        "\n",
        "# Define the loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2y0bROfqXt2",
        "outputId": "20005200-5f3d-4a93-ee9d-0ecfaa0ef294"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RegressionModel(\n",
            "  (fc1): Linear(in_features=2, out_features=500, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=500, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Define minibatch size\n",
        "batch_size = 32\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 1000\n",
        "num_batches = len(x_train) // batch_size\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Shuffle the training data for each epoch\n",
        "    indices = np.arange(len(x_train))\n",
        "    np.random.shuffle(indices)\n",
        "    x_train_shuffled = x_train[indices]\n",
        "    y_train_shuffled = y_train[indices]\n",
        "\n",
        "    for batch in range(num_batches):\n",
        "        # Get the current minibatch\n",
        "        start_idx = batch * batch_size\n",
        "        end_idx = start_idx + batch_size\n",
        "        batch_inputs = x_train_shuffled[start_idx:end_idx]\n",
        "        batch_targets = y_train_shuffled[start_idx:end_idx]\n",
        "\n",
        "        # Reshape the target tensor\n",
        "        batch_targets = batch_targets.reshape(-1, 1)\n",
        "\n",
        "        # Move the minibatch to GPU if available\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_targets = batch_targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_inputs)\n",
        "        loss = loss_function(outputs, batch_targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Adjust learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}\")\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Learning Rate: {scheduler.get_last_lr()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0dB9z9Llqr1F",
        "outputId": "7c7b2ae2-3a55-460c-b073-6ba67a6fd729"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 1183.237366442871\n",
            "Epoch [10/1000], Loss: 824.3477783203125, Learning Rate: [0.1]\n",
            "Epoch [20/1000], Loss: 1158.114089428711\n",
            "Epoch [20/1000], Loss: 1058.2532958984375, Learning Rate: [0.1]\n",
            "Epoch [30/1000], Loss: 1177.3227379516602\n",
            "Epoch [30/1000], Loss: 1188.477783203125, Learning Rate: [0.1]\n",
            "Epoch [40/1000], Loss: 1156.9727329833984\n",
            "Epoch [40/1000], Loss: 787.0048217773438, Learning Rate: [0.1]\n",
            "Epoch [50/1000], Loss: 1158.2169631835939\n",
            "Epoch [50/1000], Loss: 955.909423828125, Learning Rate: [0.1]\n",
            "Epoch [60/1000], Loss: 1140.648581201172\n",
            "Epoch [60/1000], Loss: 1159.6434326171875, Learning Rate: [0.1]\n",
            "Epoch [70/1000], Loss: 1161.7459077636718\n",
            "Epoch [70/1000], Loss: 1383.760986328125, Learning Rate: [0.1]\n",
            "Epoch [80/1000], Loss: 1151.4079425048828\n",
            "Epoch [80/1000], Loss: 1789.72412109375, Learning Rate: [0.1]\n",
            "Epoch [90/1000], Loss: 1152.9864003295897\n",
            "Epoch [90/1000], Loss: 916.0767822265625, Learning Rate: [0.1]\n",
            "Epoch [100/1000], Loss: 1132.790809387207\n",
            "Epoch [100/1000], Loss: 1341.955322265625, Learning Rate: [0.09000000000000001]\n",
            "Epoch [110/1000], Loss: 1122.948600366211\n",
            "Epoch [110/1000], Loss: 1207.33203125, Learning Rate: [0.09000000000000001]\n",
            "Epoch [120/1000], Loss: 1205.567153869629\n",
            "Epoch [120/1000], Loss: 1103.15966796875, Learning Rate: [0.09000000000000001]\n",
            "Epoch [130/1000], Loss: 1114.3267561279297\n",
            "Epoch [130/1000], Loss: 1523.456787109375, Learning Rate: [0.09000000000000001]\n",
            "Epoch [140/1000], Loss: 1126.0695950683594\n",
            "Epoch [140/1000], Loss: 919.7771606445312, Learning Rate: [0.09000000000000001]\n",
            "Epoch [150/1000], Loss: 1146.1108424926758\n",
            "Epoch [150/1000], Loss: 1356.1214599609375, Learning Rate: [0.09000000000000001]\n",
            "Epoch [160/1000], Loss: 1178.3030119140626\n",
            "Epoch [160/1000], Loss: 975.2693481445312, Learning Rate: [0.09000000000000001]\n",
            "Epoch [170/1000], Loss: 1140.6626028320313\n",
            "Epoch [170/1000], Loss: 1008.4530029296875, Learning Rate: [0.09000000000000001]\n",
            "Epoch [180/1000], Loss: 1137.385352734375\n",
            "Epoch [180/1000], Loss: 643.852783203125, Learning Rate: [0.09000000000000001]\n",
            "Epoch [190/1000], Loss: 1141.7670927490235\n",
            "Epoch [190/1000], Loss: 791.485107421875, Learning Rate: [0.09000000000000001]\n",
            "Epoch [200/1000], Loss: 1288.129505444336\n",
            "Epoch [200/1000], Loss: 1313.5263671875, Learning Rate: [0.08100000000000002]\n",
            "Epoch [210/1000], Loss: 1118.3032540405272\n",
            "Epoch [210/1000], Loss: 867.5453491210938, Learning Rate: [0.08100000000000002]\n",
            "Epoch [220/1000], Loss: 1141.2350370483398\n",
            "Epoch [220/1000], Loss: 1130.7144775390625, Learning Rate: [0.08100000000000002]\n",
            "Epoch [230/1000], Loss: 1136.111791040039\n",
            "Epoch [230/1000], Loss: 1168.8961181640625, Learning Rate: [0.08100000000000002]\n",
            "Epoch [240/1000], Loss: 1139.4525767578125\n",
            "Epoch [240/1000], Loss: 1161.0330810546875, Learning Rate: [0.08100000000000002]\n",
            "Epoch [250/1000], Loss: 1142.505759423828\n",
            "Epoch [250/1000], Loss: 1245.4287109375, Learning Rate: [0.08100000000000002]\n",
            "Epoch [260/1000], Loss: 1169.580075402832\n",
            "Epoch [260/1000], Loss: 1177.1107177734375, Learning Rate: [0.08100000000000002]\n",
            "Epoch [270/1000], Loss: 1146.6642896728515\n",
            "Epoch [270/1000], Loss: 1299.544677734375, Learning Rate: [0.08100000000000002]\n",
            "Epoch [280/1000], Loss: 1150.6357877929688\n",
            "Epoch [280/1000], Loss: 999.8464965820312, Learning Rate: [0.08100000000000002]\n",
            "Epoch [290/1000], Loss: 1104.893711657715\n",
            "Epoch [290/1000], Loss: 1613.669921875, Learning Rate: [0.08100000000000002]\n",
            "Epoch [300/1000], Loss: 1144.2924018920899\n",
            "Epoch [300/1000], Loss: 1570.26123046875, Learning Rate: [0.07290000000000002]\n",
            "Epoch [310/1000], Loss: 1133.9863633789062\n",
            "Epoch [310/1000], Loss: 1379.0052490234375, Learning Rate: [0.07290000000000002]\n",
            "Epoch [320/1000], Loss: 1128.7219290893554\n",
            "Epoch [320/1000], Loss: 912.7321166992188, Learning Rate: [0.07290000000000002]\n",
            "Epoch [330/1000], Loss: 1142.9446925048828\n",
            "Epoch [330/1000], Loss: 1179.4130859375, Learning Rate: [0.07290000000000002]\n",
            "Epoch [340/1000], Loss: 1161.8507238769532\n",
            "Epoch [340/1000], Loss: 1086.034423828125, Learning Rate: [0.07290000000000002]\n",
            "Epoch [350/1000], Loss: 1106.6181349853516\n",
            "Epoch [350/1000], Loss: 991.0935668945312, Learning Rate: [0.07290000000000002]\n",
            "Epoch [360/1000], Loss: 1202.7463470581054\n",
            "Epoch [360/1000], Loss: 890.8512573242188, Learning Rate: [0.07290000000000002]\n",
            "Epoch [370/1000], Loss: 1132.4639747192382\n",
            "Epoch [370/1000], Loss: 1648.471923828125, Learning Rate: [0.07290000000000002]\n",
            "Epoch [380/1000], Loss: 1126.3718598876953\n",
            "Epoch [380/1000], Loss: 1157.241943359375, Learning Rate: [0.07290000000000002]\n",
            "Epoch [390/1000], Loss: 1182.1544103149415\n",
            "Epoch [390/1000], Loss: 1015.345458984375, Learning Rate: [0.07290000000000002]\n",
            "Epoch [400/1000], Loss: 1141.1096126708985\n",
            "Epoch [400/1000], Loss: 679.4891967773438, Learning Rate: [0.06561000000000002]\n",
            "Epoch [410/1000], Loss: 1125.242718774414\n",
            "Epoch [410/1000], Loss: 1090.5933837890625, Learning Rate: [0.06561000000000002]\n",
            "Epoch [420/1000], Loss: 1120.0364187866212\n",
            "Epoch [420/1000], Loss: 1225.76318359375, Learning Rate: [0.06561000000000002]\n",
            "Epoch [430/1000], Loss: 1109.1390947509765\n",
            "Epoch [430/1000], Loss: 1621.717041015625, Learning Rate: [0.06561000000000002]\n",
            "Epoch [440/1000], Loss: 1105.5416362426758\n",
            "Epoch [440/1000], Loss: 1000.0283203125, Learning Rate: [0.06561000000000002]\n",
            "Epoch [450/1000], Loss: 1155.9122220947265\n",
            "Epoch [450/1000], Loss: 1157.829345703125, Learning Rate: [0.06561000000000002]\n",
            "Epoch [460/1000], Loss: 1088.5169506469726\n",
            "Epoch [460/1000], Loss: 1543.5076904296875, Learning Rate: [0.06561000000000002]\n",
            "Epoch [470/1000], Loss: 1195.7117300415039\n",
            "Epoch [470/1000], Loss: 1252.4150390625, Learning Rate: [0.06561000000000002]\n",
            "Epoch [480/1000], Loss: 1099.0985311523436\n",
            "Epoch [480/1000], Loss: 841.2318115234375, Learning Rate: [0.06561000000000002]\n",
            "Epoch [490/1000], Loss: 1099.3375252807616\n",
            "Epoch [490/1000], Loss: 684.0997314453125, Learning Rate: [0.06561000000000002]\n",
            "Epoch [500/1000], Loss: 1111.9658798461915\n",
            "Epoch [500/1000], Loss: 980.0971069335938, Learning Rate: [0.05904900000000002]\n",
            "Epoch [510/1000], Loss: 1099.890786291504\n",
            "Epoch [510/1000], Loss: 929.0531005859375, Learning Rate: [0.05904900000000002]\n",
            "Epoch [520/1000], Loss: 1082.8223493774415\n",
            "Epoch [520/1000], Loss: 967.0263671875, Learning Rate: [0.05904900000000002]\n",
            "Epoch [530/1000], Loss: 1100.9222153930664\n",
            "Epoch [530/1000], Loss: 1016.3723754882812, Learning Rate: [0.05904900000000002]\n",
            "Epoch [540/1000], Loss: 1069.1645190917968\n",
            "Epoch [540/1000], Loss: 1289.190185546875, Learning Rate: [0.05904900000000002]\n",
            "Epoch [550/1000], Loss: 1078.592570703125\n",
            "Epoch [550/1000], Loss: 1337.233154296875, Learning Rate: [0.05904900000000002]\n",
            "Epoch [560/1000], Loss: 1068.1682467163087\n",
            "Epoch [560/1000], Loss: 1146.176513671875, Learning Rate: [0.05904900000000002]\n",
            "Epoch [570/1000], Loss: 1102.934948803711\n",
            "Epoch [570/1000], Loss: 620.0982055664062, Learning Rate: [0.05904900000000002]\n",
            "Epoch [580/1000], Loss: 1140.41197064209\n",
            "Epoch [580/1000], Loss: 725.6035766601562, Learning Rate: [0.05904900000000002]\n",
            "Epoch [590/1000], Loss: 1098.7425354614259\n",
            "Epoch [590/1000], Loss: 901.57177734375, Learning Rate: [0.05904900000000002]\n",
            "Epoch [600/1000], Loss: 1082.8486314453125\n",
            "Epoch [600/1000], Loss: 840.6328125, Learning Rate: [0.05314410000000002]\n",
            "Epoch [610/1000], Loss: 1099.8538165039063\n",
            "Epoch [610/1000], Loss: 1243.7156982421875, Learning Rate: [0.05314410000000002]\n",
            "Epoch [620/1000], Loss: 1080.514666833496\n",
            "Epoch [620/1000], Loss: 910.1011352539062, Learning Rate: [0.05314410000000002]\n",
            "Epoch [630/1000], Loss: 1106.2209304199218\n",
            "Epoch [630/1000], Loss: 932.8961791992188, Learning Rate: [0.05314410000000002]\n",
            "Epoch [640/1000], Loss: 1067.5297526855468\n",
            "Epoch [640/1000], Loss: 1085.6385498046875, Learning Rate: [0.05314410000000002]\n",
            "Epoch [650/1000], Loss: 1082.4433958984375\n",
            "Epoch [650/1000], Loss: 1451.4417724609375, Learning Rate: [0.05314410000000002]\n",
            "Epoch [660/1000], Loss: 1091.5141715698242\n",
            "Epoch [660/1000], Loss: 1147.859130859375, Learning Rate: [0.05314410000000002]\n",
            "Epoch [670/1000], Loss: 1073.8762620117188\n",
            "Epoch [670/1000], Loss: 760.9598388671875, Learning Rate: [0.05314410000000002]\n",
            "Epoch [680/1000], Loss: 1068.0621275878907\n",
            "Epoch [680/1000], Loss: 866.8095092773438, Learning Rate: [0.05314410000000002]\n",
            "Epoch [690/1000], Loss: 1093.4305729125977\n",
            "Epoch [690/1000], Loss: 751.7715454101562, Learning Rate: [0.05314410000000002]\n",
            "Epoch [700/1000], Loss: 1082.916461352539\n",
            "Epoch [700/1000], Loss: 1285.328369140625, Learning Rate: [0.04782969000000002]\n",
            "Epoch [710/1000], Loss: 1092.5572453979491\n",
            "Epoch [710/1000], Loss: 1055.4385986328125, Learning Rate: [0.04782969000000002]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bc2dceec45ec>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_outputs = model(x_test)\n",
        "    test_loss_log = loss_function(test_outputs, y_test)\n",
        "    test_loss_lin = loss_function(torch.exp(test_outputs), torch.exp(y_test))\n",
        "    print(f\"Test Loss log: {test_loss_log.item()}\", f\"Test Loss lin: {test_loss_lin.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPyM0y57qzTi",
        "outputId": "463b0204-c1aa-49e3-87ba-d135363ddd61"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss log: 1813.875732421875 Test Loss lin: inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([20000])) that is different to the input size (torch.Size([20000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    }
  ]
}